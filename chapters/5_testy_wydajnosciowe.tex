\chapter{Testy wydajnościowe}
\section{Testy wydajnościowe oprogramowania}
Ważnym procesem podczas tworzenia oprogramowanie jest jego testowanie. Celem testów jest zapewnienie jakości wytwarzanego oprogramowania zgodnego ze specyfikacją. Jednym z typów takich testów są testy wydajnościowe. Testy wydajnościowe pozwalają sprawdzić, jak oprogramowanie zachowa się w zależności od obciążenia. Między innymi można zbadać czy nie nastąpi przerwa w działaniu systemu, ile czasu będą zajmować poszczególne funkcje systemu, jak wykorzystane będą zasoby systemu oraz czy pozwala na skalowanie. 

Testy wydajnościowe mogą przyjmować różną formę. Pierwszą z nich są testy obciążeniowe. W takim tescie ustawia wybiera się ilu użytkowników jednocześnie ma korzystać z aplikacji. Na podstawie dancyh otrzymanych z takiego testu można zbadać, jak aplikacja zachowa się podczas określonego obciążenia Dzięki takim testom można otrzymać informacje o czasach odpowiedzi aplikacji.

Drugą formą są testy przeciążeniowe pozwalają na określenie, jak zachowa się system w warunkach skrajnego obciążenia. Przykładem takiego testu jest zbyt duża liczba użytkowników aplikacji, korzystających w tym samym czasie. Dzięki takim testom można między zaobserwować czy i w jakim momencie aplikacja się wyłączy.

Testy wydajnościowe mogą być prowadzone w różnym celu. W aplikacjach internetowych testy przeprowadza się by zbadać, jak dużo użytkowników jest w stanie z systemu korzystać jednocześnie. Takie testy można przeprowadzić zarówno z jednego komputera jak i z kilku. 

W aplikacjach badane również mogą być czasy odpowiedzi aplikacji w zależności od liczby równoległych żądań. Zazwyczaj testy takie są stosowane, gdy aplikacja w swojej specyfikacji ma określony maksymalny czas odpowiedzi przy danej liczbie żądań.

Testy wydajnościowe powinny być prowadzone na środowisku testowym, identycznym do produkcyjnego.

\section{Opis opracowanych testów wydajnościowych}

Do testów przygotowano dwie aplikację. Pierwszą z nich była aplikacja w języku \textsl{Java} uruchomiona na dwóch różnych serwerach: \textsl{Tomcat 8} i \textsl{Jetty 9}. Serwery te są obecnie jednymi z najpopularniejszych rozwiązań służących do uruchamiania aplikacji \textsl{Java}.  Drugą aplikacją była aplikacja w języku \textsl{GO}, która przy użyciu biblioteki \textsl{HttpRouter} (https://github.com/julienschmidt/httprouter) pozwala na tworzenie aplikacji działającej jako serwer \textsl{HTTP}.

Każda aplikacja była testowana przez przygotowany zbiór testów w aplikacji \textsl{Apache JMeter}. \textsl{Apache JMeter} pozwala na tworzenie i wykonywanie testów wydajnościowych, symulujących wielu klientów korzystających z aplikacji. Przygotowane testy zostały podzielone na 4 grupy:
\begin{itemize}
    \item testy sprawdzające wydajność walidacji istnienia klucza \textsl{API}
    \item testy sprawdzające wydajność walidacji obiektu \textsl{Cache}
    \item testy sprawdzające wydajność operacji typu \textsl{CRUD}
    \item testy sprawdzające wydajność powyższych scenariuszy równolegle
\end{itemize}

Pierwsza grupa sprawdzała wydajność aplikacji, gdy klient chciał wykonać operacje posiadając nieistniejący klucz \textsl{API}. Na tą grupę składało się pięć testów, wykonywanej w następującej kolejności:
\begin{enumerate}
    \item pobieranie listy wszyskitch obiektów \textsl{Cache}
    \item pobieranie pojedynczego obiektu \textsl{Cache}
    \item tworzenie obiektu \textsl{Cache}
    \item aktualizacji obiektu \textsl{Cache} 
    \item usunięcie obiektu \textsl{Cache}
\end{enumerate}
Każdy z tych testów oznaczany był jako poprawny, gdy aplikacja zwracała błąd autoryzacji dla każdego żądania.

Drugą grupą testów było sprawdzenie wydajności, gdy klucz \textsl{API} istniał jednak klient chciał przeprowadzić operacje pobierania, usuwania i aktualizacji nie istniejącego obiektu \textsl{Cache}. Scenariusz grupy wyglądał następująco:
\begin{enumerate}
    \item pobierz nowy klucz \textsl{API}
    \item pobierz obiekt \textsl{Cache} autoryzując się otrzymanym kluczem \textsl{API}
    \item zaktualizuj obiekt \textsl{Cache} autoryzując się otrzymanym kluczem \textsl{API}
    \item usuń obiekt \textsl{Cache} autoryzując się otrzymanym kluczem \textsl{API}
\end{enumerate}
Każdy z tych testów oznaczany był jako poprawny, gdy aplikacja zwracała błąd autoryzacji dla każdego żądania.

Kolejną grupą były testy wydajności operacji \textsl{CRUD}. Do przeprowadzenia tej grupy testów został przygotowany zbiór 100 tysięcy losowych wartości w formie pliku \textsl{CSV}. Każda wartość składała się 3 części. Pierwszą był klucz obiektu \textsl{Cache}, natomiast dwie kolejne to wartości, które zostaną zapisane w aplikacji w polu \textsl{value} obiektu. \textsl{Apache JMeter} pozwala na przekazanie pliku \textsl{CSV}, którego wartości można użyć do przeprowadzenia testów. Scenariusz tej grupy wyglądał następująco:
\begin{enumerate}
    \item pobierz nowy klucz \textsl{API}
    \item utwórz obiekt \textsl{Cache} o kluczu i wartości otrzymanym z parametru
    \item pobierz utworzony obiekt \textsl{Cache} 
    \item zaktualizuj obiekt \textsl{Cache} ustawiając nową wartość pola \textsl{value}
    \item usuń obiekt \textsl{Cache}
\end{enumerate}
Każdy z tych testów oznaczany był jako poprawny, gdy aplikacja dla każdego z nich nie zwracała błędu.

Ostatnią grupę stanowiły wszystkie testy, które zostały opisane w powyższych grupach. 

Wszystkie grupy były wykonywane w 15 minutowych cyklach, oddzielonych 60 sekundową przerwą.

Każda aplikacja testowana była w czterech przypadkach testowych. Przypadki te różniły się od siebie liczbą klientów, którzy równolegle wykonywali żądania oraz stanu początkowego bazy danych:
\begin{itemize}
    \item 100 klientów oraz pusta baza danych
    \item 250 klientów oraz pusta baza danych
    \item 100 klientów oraz baza danych wypełniona danymi
    \item 250 klientów oraz baza danych wypełniona danymi
\end{itemize}
W dwóch ostatnich przypadkach baza danych była wypełniona losowymi danymi: 4000000 obiektów w kolekcji \textsl{api}, 40000000 obiektów w kolekcji \textsl{cache}. Łącznie baza danych zajmowała 12 gigabajtów pamięci masowej.

\section{Środowisko testowe}
Do przeprowadzenia testów wydajnościowych wykorzystywane były 3 serwery wirtualne. Specyfikacje techniczne serwerów wirtualnych, wykorzystanych w testach to: 
\begin{itemize}
    \item 8 rdzeni, 16 gigabajtów pamięci RAM, 160 gigabajtów dysku SSD dla serwera aplikacyjnego
    \item 4 rdzenie, 8 gigabajtów pamięci RAM, 80 gigabajtów dysku SSD dla serwera bazy danych 
    \item 4 rdzenie, 8 gigabajtów pamięci RAM, 80 gigabajtów dysku SSD dla serwera, na którym uruchomiony był program \textsl{Apache JMeter}
\end{itemize}
Serwery komunikowały się po sieci lokalnej (ang. \textsl{LAN}) bezpośrednio między sobą.

Na rysunku \ref{fig:deployment_diagram} zaprezentowany został diagram wdrożenia infrastruktury wykorzystanej do przeprowadzenia testów.
\begin{figure}[!ht]
\centering
\includegraphics[width=12cm, height=9cm]{\ImgPath/diagram_wdrozenia.png}
\caption{Diagram wdrożenia infrastruktury wykorzystywanej do przeprowadzenia testów}
\label{fig:deployment_diagram}
\end{figure}

\newpage
\section{Wyniki testów - pusta baza danych}

\subsection{Testy wydajności walidacji API}
Pierwszą grupą testów były testy wydajności walidujących istnienie kliczy \textsl{API}. Z diagramów \ref{fig:tomcat_clean_api_validation_rps} i \ref{fig:tomcat_clean_api_validation_td} przedstawiających wydajność aplikacji \textsl{Java} uruchomionej na serwerze \textsl{Tomcat 8} wynika, że przy 100 równolegle uruchomionych klientach liczba obsłużonych żądań wahała się od 6 do 8 tysięcy żądań w ciągu sekundy. Natomiast średni czas odpowiedzi wynosił 11.82 milisekund. W przypadku 250 klientów, którzy testowali aplikację liczba żądań wahała się miedzy 4 a 8 tysiącami, a rozkład czasów odpowiedzi bardzo się spłaszczył. Średni czas odpowiedzi wynosił w tym teście 33.73 milisekundy.

Wyniki testów dla aplikacji \textsl{Java} uruchomionej na serwerze \textsl{Jetty 9} przedstawione są na diagramach \ref{fig:jetty_clean_api_validation_rps} i \ref{fig:jetty_clean_api_validation_td}. W teście przy 100 klientach liczba obsłużonych żądań w ciągu sekundy wahała się od 7 do 10 tysięcy, a średni czas odpowiedzi wyniósł 9.30 milisekund. Na diagramie rozkładu czasów odpowiedzi widać, że największa ilość żądań trwała poniżej 10 milisekund. Dla testu z uruchomionymi 250 klientami liczba żądań wahała się między 5 a 10 tysięcy i rozkład czasów również się spłaszczył. W teście tym średni czas odpowiedzi wyniósł 33.66 milisekund. 

Ostatnią testowaną aplikacją była aplikacja napisana w języku \textsl{Go}. Rozkład liczby żądań obsłużonych w ciągu sekundy (\ref{fig:go_clean_api_validation_rps}) dla testu z wykorzystaniem 100 i 250 klientów jest bardzo zbliżony. Dla 100 klientów oscyluje w okolicy 10 tysięcy żądań, dla 250 klientów w okolicy 9 tysięcy żądań. Analizując rozkład czasów odpowiedzi (\ref{fig:go_clean_api_validation_td}) można zauważyć, że w obu testach jest on symetryczny. Średnie czasy odpowiedzi wyniosły: 8.31 milisekund dla 100 klientów oraz 23.80 milisekund dla 250 klientów.

W tabelach \ref{tab:app-clean-api} oraz \ref{tab:mongo-clean-api} zaprezentowane są średnie wykorzystanie procesora oraz pamięci \textsl{RAM} na maszynach, gdzie uruchomione były aplikacje oraz baza danych \textsl{MongoD}. Aplkacja \textsl{Java} uruchomiona na serwerze \textsl{Tomcat 8} wykorzystywała najbardziej zasoby procesora (77.50\% i 65.73\%), serwer \textsl{Jetty 9} obciążał o około 5\% mniej procesor. Natomiast aplikacja w języku \textsl{Go} wymagała najmniej zasobów procesora (29.93\% i 31.54\%). Serwer \textsl{Jetty 9} potrzebował najwięcej pamięci \textsl{RAM} do działania (ponad 2000 MB), \textsl{Tomcat 8} potrzebował o 100 MB mniej, a aplikacja w języku \textsl{Go} potrzebowała około 210 MB pamięci. Roœnież serwer bazy danych był mniej obciążony w teście aplikacji napisanej w języku \textsl{Go}.

\input{chapters/5_testy_wydajnosciowe_diagram_1_clean_api_validation.tex}
\input{chapters/5_testy_wydajnosciowe_tabela_1_clean_api_validation.tex}
\clearpage

\subsection{Test wydajności walidacji istnienia obiektów Cache}
Kolejną grupą testów były testy wydajności walidacji istnienia obiektów \textsl{Cache}. Diagramy \ref{fig:tomcat_clean_key_validation_rps},  \ref{fig:jetty_clean_key_validation_rps} oraz  \ref{fig:go_clean_key_validation_rps} prezentują rozkład ilości żądań obsługiwanych przez poszczególne aplikacje podczas trwania testu. W przypadku testu z użyciem 100 klientów największą wydajność miała aplikacja napisana w \textsl{Go}. Była ona w stanie obsłużyć ponad 7 tysięcy żądań na sekundę. Serwer \textsl{Jetty 9} z aplikacją \textsl{Java} był w stanie osiągnąć przepustowość od 3.5 do 4.5 tysiąca żądań na sekundę. Najmniejszą przepustowość osiągnął w teście serwer \textsl{Tomcat 8} osiągając wartości między 2.5 a 4 tysiące żądań na sekundę. W przypadku 250 klientów również najlepszy wynik osiągnęła aplikacja w \textsl{Go}. Jej przepustowość była niewiele mniejsza od testu ze stoma klientami. Serwery \textsl{Jetty 9} i \textsl{Tomcat 8} osiągnęły wyniki zbliżone do siebie, odpowiednio: około 3 tysiące i y żądań. Dodatkowo serwer \textsl{Tomcat 8} miał większe wachania przepustowości podczas testu.

Z diagramów rozkładu czasów odpowiedzi (\ref{fig:tomcat_clean_key_validation_td},  \ref{fig:jetty_clean_key_validation_td}, \ref{fig:go_clean_key_validation_td}) wynika, że podczas próby 100 klientów najlepsze wyniki osiągneła aplikacja w \textsl{Go}, gdzie najdłużej trwające żądania trwały 25 milisekund, a średnia wyniosła 12.80 milisekund. Rozkład czasów serwera \textsl{Jetty} był bardzo równy, a średnia czasów odpowiedzi wyniosła 21.55 milisekund. Najdłuższe czasy zanotowano w teście serwera \textsl{Tomcat 8}, gdzie średnia odpowiedzi wyniosła 26.73 milisekundy, jednak najdłuższe żądania trwały 120 milisekund podczas testu. W teście z 250 klientami również najmniejsza średnia jest w aplikacji napisanej w \textsl{Go} (30.43 milisekundy). Następny był serwer \textsl{Jetty 9} ze średnią 76.41 milisekund, a najdłużej żądania obsługiwał serwer \textsl{Tomcat 8} (80.19 milisekund). Rozkład czasów przy 250 klientach był bardziej, w porównaniu do testu ze stoma klientami.

Tabele \ref{tab:app-clean-key} oraz \ref{tab:mongo-clean-key} prezentują średnie wykorzystanie procesora i pamięci \textsl{RAM} podczas testów. W przypadku testu symulującego 100 klientów równolegle serwery \textsl{Tomcat} i \textsl{Jetty} wykorzystywały podobnie zasoby (50.60\% oraz 1940 MB pamięci). Aplikacja w \textsl{Go} wykorzystywała procesor średnio na poziomie \textsl{39.28\%}, a zapotrzebowanie na pamięć \textsl{RAM} było prawie 10 razy mniejsze niż w przypadku aplikacji w języku \textsl{Java}. W przypadku testu z 250 klientami wykorzystanie pamięci \textsl{RAM} wzrosło w przypadku serwera \textsl{Tomcat 8} do 2009.80 MB, 234.03 MB w aplikacji w \textsl{Go}, a w przypadku serwera \textsl{Jetty 9} zmalało do 1851.88 MB.\\*
Podczas testowania serwera \textsl{Jetty} przy obciążeniu 100 klientów wykorzystanie procesora serwera z bazą danych było znacznie wyższe (58.65\%) niż w w przypadków testowych (około 44\%).  W pozostałych testach serwer był obciążony w zbliżony sposób: od 40\% do 45\%.
 
\input{chapters/5_testy_wydajnosciowe_diagram_2_clean_key_validation.tex}
\input{chapters/5_testy_wydajnosciowe_tabela_2_clean_key_validation.tex}
\clearpage

\subsection{Test wydajności operacji CRUD}
% \input{chapters/5_testy_wydajnosciowe_diagram_3_clean_crud.tex}
% \input{chapters/5_testy_wydajnosciowe_tabela_3_clean_crud.tex}
\clearpage

\subsection{Test wydajności walidacji API, obiektów Cache oraz operacji CRUD równolegle}
% \input{chapters/5_testy_wydajnosciowe_diagram_4_clean_all.tex}
% \input{chapters/5_testy_wydajnosciowe_tabela_4_clean_all.tex}
\clearpage

\newpage
\section{Wyniki testów - baza danych wypełniona danymi początkowymi}
\subsection{Testy wydajnośsci walidacji API}
% \input{chapters/5_testy_wydajnosciowe_diagram_5_full_api_validation.tex}
% \input{chapters/5_testy_wydajnosciowe_tabela_5_full_api_validation.tex}
\clearpage

\subsection{Test wydajności walidacji istnienia obiektów Cache}
% \input{chapters/5_testy_wydajnosciowe_diagram_6_full_key_validation.tex}
% \input{chapters/5_testy_wydajnosciowe_tabela_6_full_key_validation.tex}
\clearpage

\subsection{Test wydajności operacji CRUD}
% \input{chapters/5_testy_wydajnosciowe_diagram_7_full_crud.tex}
% \input{chapters/5_testy_wydajnosciowe_tabela_7_full_crud.tex}
\clearpage

\subsection{Test wydajności walidacji API, obiektów Cache oraz operacji CRUD równolegle }
% \input{chapters/5_testy_wydajnosciowe_diagram_8_full_all.tex}
% \input{chapters/5_testy_wydajnosciowe_tabela_8_full_all.tex}
\clearpage

\newpage
\section{Podsumowanie wyników}
