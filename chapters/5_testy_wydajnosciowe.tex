\chapter{Testy wydajnościowe}
\section{Opis testów}

Do testów przygotowano dwie aplikację. Pierwszą z nich była aplikacja w języku \textsl{Java} uruchomiona na dwóch różnych serwerach: \textsl{Tomcat 8} i \textsl{Jetty 9}. Serwery te są obecnie jednymi z najpopularniejszych rozwiązań służących do uruchamiania aplikacji \textsl{Java}.  Drugą aplikacją była aplikacja w języku \textsl{GO}, która przy użyciu biblioteki \textsl{HttpRouter} (https://github.com/julienschmidt/httprouter) pozwala na tworzenie aplikacji działającej jako serwer \textsl{HTTP}.

Każda aplikacja była testowana przez przygotowany zbiór testów w aplikacji \textsl{Apache JMeter}. \textsl{Apache JMeter} pozwala na tworzenie i wykonywanie testów wydajnościowych, symulujących wielu klientów korzystających z aplikacji. Przygotowane testy zostały podzielone na 4 grupy:
\begin{itemize}
    \item testy sprawdzające wydajność walidacji istnienia klucza \textsl{API}
    \item testy sprawdzające wydajność walidacji obiektu \textsl{Cache}
    \item testy sprawdzające wydajność operacji typu \textsl{CRUD}
    \item testy sprawdzające wydajność powyższych scenariuszy równolegle
\end{itemize}

Pierwsza grupa sprawdzała wydajność aplikacji, gdy klient chciał wykonać operacje posiadając nieistniejący klucz \textsl{API}. Na tą grupę składało się pięć testów, wykonywanej w następującej kolejności:
\begin{enumerate}
    \item pobieranie listy wszyskitch obiektów \textsl{Cache}
    \item pobieranie pojedynczego obiektu \textsl{Cache}
    \item tworzenie obiektu \textsl{Cache}
    \item aktualizacji obiektu \textsl{Cache} 
    \item usunięcie obiektu \textsl{Cache}
\end{enumerate}
Każdy z tych testów oznaczany był jako poprawny, gdy aplikacja zwracała błąd autoryzacji dla każdego żądania.

Drugą grupą testów było sprawdzenie wydajności, gdy klucz \textsl{API} istniał jednak klient chciał przeprowadzić operacje pobierania, usuwania i aktualizacji nie istniejącego obiektu \textsl{Cache}. Scenariusz grupy wyglądał następująco:
\begin{enumerate}
    \item pobierz nowy klucz \textsl{API}
    \item pobierz obiekt \textsl{Cache} autoryzując się otrzymanym kluczem \textsl{API}
    \item zaktualizuj obiekt \textsl{Cache} autoryzując się otrzymanym kluczem \textsl{API}
    \item usuń obiekt \textsl{Cache} autoryzując się otrzymanym kluczem \textsl{API}
\end{enumerate}
Każdy z tych testów oznaczany był jako poprawny, gdy aplikacja zwracała błąd autoryzacji dla każdego żądania.

Kolejną grupą były testy wydajności operacji \textsl{CRUD}. Do przeprowadzenia tej grupy testów został przygotowany zbiór 100 tysięcy losowych wartości w formie pliku \textsl{CSV}. Każda wartość składała się 3 części. Pierwszą był klucz obiektu \textsl{Cache}, natomiast dwie kolejne to wartości, które zostaną zapisane w aplikacji w polu \textsl{value} obiektu. \textsl{Apache JMeter} pozwala na przekazanie pliku \textsl{CSV}, którego wartości można użyć do przeprowadzenia testów. Scenariusz tej grupy wyglądał następująco:
\begin{enumerate}
    \item pobierz nowy klucz \textsl{API}
    \item utwórz obiekt \textsl{Cache} o kluczu i wartości otrzymanym z parametru
    \item pobierz utworzony obiekt \textsl{Cache} 
    \item zaktualizuj obiekt \textsl{Cache} ustawiając nową wartość pola \textsl{value}
    \item usuń obiekt \textsl{Cache}
\end{enumerate}
Każdy z tych testów oznaczany był jako poprawny, gdy aplikacja dla każdego z nich nie zwracała błędu.

Ostatnią grupę stanowiły wszystkie testy, które zostały opisane w powyższych grupach. 

Wszystkie grupy były wykonywane w 15 minutowych cyklach, oddzielonych 60 sekundową przerwą.

Każda aplikacja testowana była w czterech przypadkach testowych. Przypadki te różniły się od siebie liczbą klientów, którzy równolegle wykonywali żądania oraz stanu początkowego bazy danych:
\begin{itemize}
    \item 100 klientów oraz pusta baza danych
    \item 250 klientów oraz pusta baza danych
    \item 100 klientów oraz baza danych wypełniona danymi
    \item 250 klientów oraz baza danych wypełniona danymi
\end{itemize}
W dwóch ostatnich przypadkach baza danych była wypełniona losowymi danymi: 4000000 obiektów w kolekcji \textsl{api}, 40000000 obiektów w kolekcji \textsl{cache}. Łącznie baza danych zajmowała 12 gigabajtów pamięci masowej.

\section{Środowisko testowe}
Do przeprowadzenia testów wydajnościowych wykorzystywane były 3 serwery wirtualne. Specyfikacje techniczne serwerów wirtualnych, wykorzystanych w testach to: 
\begin{itemize}
    \item 8 rdzeni, 16 gigabajtów pamięci RAM, 160 gigabajtów dysku SSD dla serwera aplikacyjnego
    \item 4 rdzenie, 8 gigabajtów pamięci RAM, 80 gigabajtów dysku SSD dla serwera bazy danych 
    \item 4 rdzenie, 8 gigabajtów pamięci RAM, 80 gigabajtów dysku SSD dla serwera, na którym uruchomiony był program \textsl{Apache JMeter}
\end{itemize}
Serwery komunikowały się po sieci lokalnej (ang. \textsl{LAN}) bezpośrednio między sobą.

Na rysunku \ref{fig:deployment_diagram} zaprezentowany został diagram wdrożenia infrastruktury wykorzystanej do przeprowadzenia testów.
\begin{figure}[!ht]
\centering
\includegraphics[width=12cm, height=9cm]{\ImgPath/diagram_wdrozenia.png}
\caption{Diagram wdrożenia infrastruktury wykorzystywanej do przeprowadzenia testów}
\label{fig:deployment_diagram}
\end{figure}

\newpage
\section{Wyniki testów - pusta baza danych}

\subsection{Testy wydajności walidacji API}
Pierwszą grupą testów były testy walidacji \textsl{API}. Z diagramów \ref{fig:tomcat_clean_api_validation_rps} i \ref{fig:tomcat_clean_api_validation_td} przedstawiających wydajność aplikacji \textsl{Java} uruchomionej na serwerze \textsl{Tomcat 8} wynika, że przy 100 równolegle uruchomionych klientach liczba obsłużonych żądań wahała się od 6 do 8 tysięcy żądań w ciągu sekundy. Natomiast średni czas odpowiedzi wynosił 11.82 milisekund. W przypadku 250 klientów, którzy testowali aplikację liczba żądań wahała się miedzy 4 a 8 tysiącami, a rozkład czasów odpowiedzi bardzo się spłaszczył. Średni czas odpowiedzi wynosił w tym teście 33.73 milisekundy.

Wyniki testów dla aplikacji \textsl{Java} uruchomionej na serwerze \textsl{Jetty 9} przedstawione są na diagramach \ref{fig:jetty_clean_api_validation_rps} i \ref{fig:jetty_clean_api_validation_td}. W teście przy 100 klientach liczba obsłużonych żądań w ciągu sekundy wahała się od 7 do 10 tysięcy, a średni czas odpowiedzi wyniósł 9.30 milisekund. Na diagramie rozkładu czasów odpowiedzi widać, że największa ilość żądań trwała poniżej 10 milisekund. Dla testu z uruchomionymi 250 klientami liczba żądań wahała się między 5 a 10 tysięcy i rozkład czasów również się spłaszczył. W teście tym średni czas odpowiedzi wyniósł 33.66 milisekund. 

Ostatnią testowaną aplikacją była aplikacja napisana w języku \textsl{Go}. Rozkład liczby żądań obsłużonych w ciągu sekundy (\ref{fig:go_clean_api_validation_rps}) dla testu z wykorzystaniem 100 i 250 klientów jest bardzo zbliżony. Dla 100 klientów oscyluje w okolicy 10 tysięcy żądań, dla 250 klientów w okolicy 9 tysięcy żądań. Analizując rozkład czasów odpowiedzi (\ref{fig:go_clean_api_validation_td}) można zauważyć, że w obu testach jest on symetryczny. Średnie czasy odpowiedzi wyniosły: 8.31 milisekund dla 100 klientów oraz 23.80 milisekund dla 250 klientów.

W tabelach \ref{tab:app-clean-api} oraz \ref{tab:mongo-clean-api} zaprezentowane są średnie wykorzystanie procesora oraz pamięci \textsl{RAM} na maszynach, gdzie uruchomione były aplikacje oraz baza danych \textsl{MongoD}. Aplkacja \textsl{Java} uruchomiona na serwerze \textsl{Tomcat 8} wykorzystywała najbardziej zasoby procesora (77.50\% i 65.73\%), serwer \textsl{Jetty 9} obciążał o około 5\% mniej procesor. Natomiast aplikacja w języku \textsl{Go} wymagała najmniej zasobów procesora (29.93\% i 31.54\%). Serwer \textsl{Jetty 9} potrzebował najwięcej pamięci \textsl{RAM} do działania (ponad 2000 MB), \textsl{Tomcat 8} potrzebował o 100 MB mniej, a aplikacja w języku \textsl{Go} potrzebowała około 210 MB pamięci. 

% \input{chapters/5_testy_wydajnosciowe_diagram_1_clean_api_validation.tex}
\input{chapters/5_testy_wydajnosciowe_tabela_1_clean_api_validation.tex}
\clearpage

\subsection{Test wydajności walidacji kluczy w bazie}
% \input{chapters/5_testy_wydajnosciowe_diagram_2_clean_key_validation.tex}
\input{chapters/5_testy_wydajnosciowe_tabela_2_clean_key_validation.tex}
\clearpage

\subsection{Test wydajności operacji CRUD}
% \input{chapters/5_testy_wydajnosciowe_diagram_3_clean_crud.tex}
\input{chapters/5_testy_wydajnosciowe_tabela_3_clean_crud.tex}
\clearpage

\subsection{Test wydajności walidacji API, walidacji kluczy w bazie oraz operacji CRUD rówlolegle }
% \input{chapters/5_testy_wydajnosciowe_diagram_4_clean_all.tex}
\input{chapters/5_testy_wydajnosciowe_tabela_4_clean_all.tex}
\clearpage

\newpage
\section{Wyniki testów - baza danych wypełniona danymi początkowymi}
\subsection{Testy wydajnośsci walidacji API}
% \input{chapters/5_testy_wydajnosciowe_diagram_5_full_api_validation.tex}
\input{chapters/5_testy_wydajnosciowe_tabela_5_full_api_validation.tex}
\clearpage

\subsection{Test wydajności walidacji kluczy w bazie}
% \input{chapters/5_testy_wydajnosciowe_diagram_6_full_key_validation.tex}
\input{chapters/5_testy_wydajnosciowe_tabela_6_full_key_validation.tex}
\clearpage

\subsection{Test wydajności operacji CRUD}
% \input{chapters/5_testy_wydajnosciowe_diagram_7_full_crud.tex}
\input{chapters/5_testy_wydajnosciowe_tabela_7_full_crud.tex}
\clearpage

\subsection{Test wydajności walidacji API, walidacji kluczy w bazie oraz operacji CRUD równolegle }
% \input{chapters/5_testy_wydajnosciowe_diagram_8_full_all.tex}
\input{chapters/5_testy_wydajnosciowe_tabela_8_full_all.tex}
\clearpage

\newpage
\section{Podsumowanie wyników}
